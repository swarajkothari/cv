<!DOCTYPE html>
<html>
    <head>
        <meta charset="utf-8">
        <title> Swaraj's profile</title>
        <link rel="stylesheet" href="css/styles.css">
        <link rel="icon" href="favicon.ico">
    </head>
    <body>
        
        <img src="images/swaraj_pp.jpg" alt="swaraj profile picture" height="300px" width="230px">
        <br>
        <a href="https://www.linkedin.com/in/swaraj-kothari-37a1a813a/"><h3>linkedin</h3></a>
        <h1> Swaraj Kothari</h1>
        <p class="no_top_padding">Big Data Engineer</p>
        <h3> Profile Summary </h3>
        <hr>
        <p>Big Data Engineer proficient in data/application integration, data warehousing, data analysis. Good in applying machine learning algorithms to get much precise predictions. Experienced in python and spark for big data engineering, machine learning and data analysis.</p>
        <h3> Professional Experience</h3>
        <hr>
        <p> Data Analytics <strong>InfoObjects,Jaipur</strong></p>

        <ul> Projects
            <ul> 
                <li>PX Kafka Streaming </li>
                <ul>
                    <li>Develop a kafka consumer job which fetch incremental data from 17 topics in every hour.</li>
                    <li>Checkpointing done in DynamoDB and saved raw encrypted data in AWS S3 bucket to make model robust and redundant.</li>
                    <li>Raw data were ingested into delta tables and then into bronze layer based on query based dagster job. All the jobs ran in databricks cluster and orchestrated through dagster.</li>
                    <li>Cloud Platform used - Databricks, AWS S3 & DynamoDB, Kafka,Dagster , Technologies used- Pyspark, delta table, SQL</li>
                </ul>

                <li>Salesforce Historical load into delta table</li>
                <ul>
                    <li>Build a file ingestion job which processes more than 6TB of data, extracted, validated and ingested into delta table.</li>
                    <li>Build a consolidated logic in Scala which splits the dataframe into chunks which are all smaller than 50GB.</li>
                    <li>Once the file is in the endpoint location AWS lambda will get triggered and then all the files will get processed to delta table as per the naming convention defined in config file.</li>
                    <li>Cloud Platform used - Databricks, AWS lambda & S3, Gitlab , Technologies used- Delta table, Scala</li>
                </ul>
            </ul>
        </ul>
        
        <h3> Academic </h3>
        <hr>
        <table border="2">
            <thead>
                <tr>
                    <th> Passing Year</th>
                    <th> Qualification</th>
                    <th> School/College</th>
                    <th> University</th>
                    <th> Grades</th>
                </tr>
            </thead>
            <tbody>
                <tr>
                    <td>2020</td>
                    <td>M.Tech (Big Data Analytics)</td>
                    <td>Vellore Institute of Technology, Vellore(T.N.)</td>
                    <td>VIT</td>
                    <td>8.68 CGPA</td>
                </tr>
                <tr>
                    <td>2018</td>
                    <td>Bachelor of Engineering (CSE)</td>
                    <td>DYPIEMR, Pune(M.H.)</td>
                    <td>SPPU</td>
                    <td>61.6%</td>
                </tr>
                <tr>
                    <td>2014</td>
                    <td>12th</td>
                    <td>Chavara Vidyapeeth, Narsinghpur (M.P.)</td>
                    <td>CBSE</td>
                    <td>70%</td>
                </tr>
            </tbody>
        </table>
        
        <h3><a href = "certifications.html"> Certifications </a></h3>
        <hr>



    </body>
</html>